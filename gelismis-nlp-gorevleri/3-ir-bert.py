# import library
from transformers import BertTokenizer, BertModel 

import numpy as np 
from sklearn.metrics.pairwise import cosine_similarity


# tokenizer and model create
model_name = "bert-base-uncased" # kucuk boyutlu bert modeli
tokenizer = BertTokenizer.from_pretrained(model_name) # tokenizer yukle
model = BertModel.from_pretrained(model_name) # onceden egitilmis bert modeli

# veri olustur: karsilastirilacak belgeleri ve sorgu cumlemizi olustur

documents = [
    "Machine learning is a field of artificial intelligence",
    "Natural language processing involves understanding human language",
    "Artificial intelligence encomppases machine learning and natural language processing (nlp)",
    "Deep learning is a subset of machine learning",
    "Data science combines statistics, adta analysis and machine learning",
    "I go to shop"
 ]

query = "What is deep learning?"

# bert ile bilgi getirimi

def get_embedding(text):
    
    # tokenization
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    
    # modeli calistir
    outputs = model(**inputs)
    
    # son gizli katmani alalim
    last_hidden_state = outputs.last_hidden_state

    # metni temsili olustur
    embedding = last_hidden_state.mean(dim=1)
    
    # vektoru numpy olarak return et
    return embedding.detach().numpy()

# belgeler ve sorgu icin embedding vektorlerini al
doc_embeddings = np.vstack([get_embedding(doc) for doc in documents])
query_embedding = get_embedding(query)

# kosinus benzerligi ile belgeler arasinda benzerligi hesaplayalim
similarities = cosine_similarity(query_embedding, doc_embeddings)

# her belgenin benzerlik skoru
for i, score in enumerate(similarities[0]):
    print(f"Document: {documents[i]} \n{score}")


most_similar_index = similarities.argmax()

print(f"Most similar document: {documents[most_similar_index]}")